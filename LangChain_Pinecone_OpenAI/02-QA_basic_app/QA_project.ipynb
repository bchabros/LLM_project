{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ChatGPT APP with LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic prompt conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd '/Users/Chabi/Documents/LLM_project/LangChain_Pinecone_OpenAI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Studio Code (VS Code) and PyCharm are both popular code editors/IDEs used by developers for programming and code editing, but they have some key differences:\n",
      "\n",
      "1. Purpose:\n",
      "   - VS Code: It is a lightweight code editor developed by Microsoft, which supports a wide range of programming languages. It is highly extensible and customizable, suitable for developers working on multiple languages and projects.\n",
      "   - PyCharm: It is a dedicated Python IDE developed by JetBrains. It provides a comprehensive set of tools specifically tailored for Python development, making it ideal for Python-only projects.\n",
      "\n",
      "2. Features:\n",
      "   - VS Code: It has rich features like IntelliSense code completion, debugging, Git integration, built-in terminal, version control, extensions marketplace, and support for multiple programming languages.\n",
      "   - PyCharm: It offers all the features of VS Code plus language-specific features like advanced code analysis, refactoring, testing frameworks integration, Python-specific debugging, virtual environments management, and Django web development support.\n",
      "\n",
      "3. Performance:\n",
      "   - VS Code: It is known for being lightweight and fast. It launches quickly and consumes fewer system resources, making it suitable for low-end machines and quick editing tasks.\n",
      "   - PyCharm: It is a more resource-intensive IDE, which can be slower on lower-end machines. However, it makes up for it with its extensive feature set specifically designed for Python development.\n",
      "\n",
      "4. Learning Curve:\n",
      "   - VS Code: Its minimalist interface and flexible nature make it easy to get started. It has a shallow learning curve, suitable for beginners or developers who prefer a simpler interface.\n",
      "   - PyCharm: With a larger feature set and more advanced tools, PyCharm has a steeper learning curve compared to VS Code. It provides more functionality, making it more suitable for experienced Python developers.\n",
      "\n",
      "In summary, VS Code is a versatile and lightweight code editor suitable for various programming languages, while PyCharm is a powerful IDE specifically designed for Python development. The choice between them often depends on the specific requirements and preferences of the programmer.\n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo',\n",
    "                 temperature=1)\n",
    "prompt = ChatPromptTemplate(input_variables=['content'],\n",
    "                            messages=[SystemMessage(content='You are chatbot having a conversation with a human.'),\n",
    "                                      HumanMessagePromptTemplate.from_template('{content}')])\n",
    "\n",
    "chain = LLMChain(llm=llm,\n",
    "                 prompt=prompt,\n",
    "                 verbose=False)\n",
    "\n",
    "while True:\n",
    "    content = input('Your prompt: ')\n",
    "    if content in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "\n",
    "    response = chain.run({'content': content})\n",
    "    print(response)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are chatbot having a conversation with a human.\n",
      "Human: Explain what is transformers in modeling (deep learning)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Transformers refer to a deep learning model architecture that has been widely used for various natural language processing (NLP) tasks such as machine translation, language understanding, and text generation. They have gained popularity due to their ability to capture long-range dependencies and handle sequential data efficiently.\n",
      "\n",
      "The key innovation of the transformer model lies in its attention mechanism, which enables the model to focus on different parts of the input sequence during the encoding and decoding process. Unlike traditional recurrent neural networks (RNNs) that process input sequentially, transformers can process all input positions in parallel, making them more efficient for handling long sequences.\n",
      "\n",
      "The transformer model consists of an encoder and a decoder. The encoder receives the input sequence and processes it layer by layer, creating a contextualized representation of each input token. The decoder, on the other hand, takes the encoder's output as well as the previously generated tokens during decoding to generate the output sequence.\n",
      "\n",
      "The main component in the attention mechanism of transformers is the self-attention mechanism, which allows each input position to attend to all other positions to compute its representation. This self-attention mechanism helps capture dependencies between all tokens in the input sequence, regardless of their distances.\n",
      "\n",
      "By stacking multiple layers of self-attention and feed-forward neural networks, transformers can learn complex representations of input sequences and effectively model linguistic relationships. They have demonstrated superior performance on various NLP tasks and have become a popular choice for researchers and practitioners in the field of deep learning.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are chatbot having a conversation with a human.\n",
      "Human: Explain what is transformers in modeling (deep learning)\n",
      "AI: Transformers refer to a deep learning model architecture that has been widely used for various natural language processing (NLP) tasks such as machine translation, language understanding, and text generation. They have gained popularity due to their ability to capture long-range dependencies and handle sequential data efficiently.\n",
      "\n",
      "The key innovation of the transformer model lies in its attention mechanism, which enables the model to focus on different parts of the input sequence during the encoding and decoding process. Unlike traditional recurrent neural networks (RNNs) that process input sequentially, transformers can process all input positions in parallel, making them more efficient for handling long sequences.\n",
      "\n",
      "The transformer model consists of an encoder and a decoder. The encoder receives the input sequence and processes it layer by layer, creating a contextualized representation of each input token. The decoder, on the other hand, takes the encoder's output as well as the previously generated tokens during decoding to generate the output sequence.\n",
      "\n",
      "The main component in the attention mechanism of transformers is the self-attention mechanism, which allows each input position to attend to all other positions to compute its representation. This self-attention mechanism helps capture dependencies between all tokens in the input sequence, regardless of their distances.\n",
      "\n",
      "By stacking multiple layers of self-attention and feed-forward neural networks, transformers can learn complex representations of input sequences and effectively model linguistic relationships. They have demonstrated superior performance on various NLP tasks and have become a popular choice for researchers and practitioners in the field of deep learning.\n",
      "Human: Explain what is transformers in modeling (deep learning)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Transformers in deep learning refer to a popular model architecture used for various natural language processing (NLP) tasks. They have revolutionized NLP and have achieved state-of-the-art results on tasks such as machine translation, text summarization, question answering, and more.\n",
      "\n",
      "Transformers were introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. The key idea behind transformers is the replacement of traditional recurrent neural networks (RNNs) with a self-attention mechanism. This allows the model to attend to different parts of the input sequence simultaneously, capturing dependencies and patterns across the entire sequence.\n",
      "\n",
      "The transformer model consists of two main components, the encoder and the decoder. The encoder takes an input sequence and processes it in parallel, creating a representation for each input token. The decoder, on the other hand, generates the output sequence by attending to the encoder's output while considering the previously generated tokens. The use of self-attention in both the encoder and the decoder allows the model to capture contextual relationships and dependencies effectively.\n",
      "\n",
      "The self-attention mechanism allows each token to weigh the importance of other tokens in the sequence when computing its representation. This attention mechanism is different from traditional RNNs, where information flows sequentially. Transformers can exploit global dependencies, making them particularly effective for long-range relationships and handling long sequences.\n",
      "\n",
      "Transformers have several benefits compared to traditional models. They parallelize well, enabling faster training and inference. They can capture long-range dependencies and maintain important contextual information, often resulting in better performance. Additionally, transformers have been augmented with techniques like pre-training and fine-tuning on large-scale datasets, leading to the development of state-of-the-art models like BERT, GPT, and T5.\n",
      "\n",
      "Due to their effectiveness and versatility, transformers have become the de facto choice for many NLP tasks. Researchers and practitioners extensively use variants and adaptations of transformer-based models to solve complex language-related problems, making them a cornerstone of modern deep learning in NLP.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are chatbot having a conversation with a human.\n",
      "Human: Explain what is transformers in modeling (deep learning)\n",
      "AI: Transformers refer to a deep learning model architecture that has been widely used for various natural language processing (NLP) tasks such as machine translation, language understanding, and text generation. They have gained popularity due to their ability to capture long-range dependencies and handle sequential data efficiently.\n",
      "\n",
      "The key innovation of the transformer model lies in its attention mechanism, which enables the model to focus on different parts of the input sequence during the encoding and decoding process. Unlike traditional recurrent neural networks (RNNs) that process input sequentially, transformers can process all input positions in parallel, making them more efficient for handling long sequences.\n",
      "\n",
      "The transformer model consists of an encoder and a decoder. The encoder receives the input sequence and processes it layer by layer, creating a contextualized representation of each input token. The decoder, on the other hand, takes the encoder's output as well as the previously generated tokens during decoding to generate the output sequence.\n",
      "\n",
      "The main component in the attention mechanism of transformers is the self-attention mechanism, which allows each input position to attend to all other positions to compute its representation. This self-attention mechanism helps capture dependencies between all tokens in the input sequence, regardless of their distances.\n",
      "\n",
      "By stacking multiple layers of self-attention and feed-forward neural networks, transformers can learn complex representations of input sequences and effectively model linguistic relationships. They have demonstrated superior performance on various NLP tasks and have become a popular choice for researchers and practitioners in the field of deep learning.\n",
      "Human: Explain what is transformers in modeling (deep learning)\n",
      "AI: Transformers in deep learning refer to a popular model architecture used for various natural language processing (NLP) tasks. They have revolutionized NLP and have achieved state-of-the-art results on tasks such as machine translation, text summarization, question answering, and more.\n",
      "\n",
      "Transformers were introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. The key idea behind transformers is the replacement of traditional recurrent neural networks (RNNs) with a self-attention mechanism. This allows the model to attend to different parts of the input sequence simultaneously, capturing dependencies and patterns across the entire sequence.\n",
      "\n",
      "The transformer model consists of two main components, the encoder and the decoder. The encoder takes an input sequence and processes it in parallel, creating a representation for each input token. The decoder, on the other hand, generates the output sequence by attending to the encoder's output while considering the previously generated tokens. The use of self-attention in both the encoder and the decoder allows the model to capture contextual relationships and dependencies effectively.\n",
      "\n",
      "The self-attention mechanism allows each token to weigh the importance of other tokens in the sequence when computing its representation. This attention mechanism is different from traditional RNNs, where information flows sequentially. Transformers can exploit global dependencies, making them particularly effective for long-range relationships and handling long sequences.\n",
      "\n",
      "Transformers have several benefits compared to traditional models. They parallelize well, enabling faster training and inference. They can capture long-range dependencies and maintain important contextual information, often resulting in better performance. Additionally, transformers have been augmented with techniques like pre-training and fine-tuning on large-scale datasets, leading to the development of state-of-the-art models like BERT, GPT, and T5.\n",
      "\n",
      "Due to their effectiveness and versatility, transformers have become the de facto choice for many NLP tasks. Researchers and practitioners extensively use variants and adaptations of transformer-based models to solve complex language-related problems, making them a cornerstone of modern deep learning in NLP.\n",
      "Human: It is the most advanced algorithm in deep learning?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "While transformers are a highly influential and widely used model architecture in deep learning, it is important to note that no single algorithm can be considered the \"most advanced\" in deep learning. Deep learning is a rapidly evolving field with researchers constantly developing and refining new algorithms, architectures, and techniques.\n",
      "\n",
      "Transformers have demonstrated impressive performance on various NLP tasks and have achieved state-of-the-art results in many cases. They have greatly improved the ability to model and process sequential data, particularly in natural language processing. However, other advanced algorithms exist in deep learning for different domains and tasks. For instance, in computer vision, convolutional neural networks (CNNs) have been instrumental in achieving significant breakthroughs.\n",
      "\n",
      "Moreover, advancements in deep learning are often driven by combinations of different algorithms and techniques. For example, models like BERT (Bidirectional Encoder Representations from Transformers) combine transformers with pre-training and fine-tuning, further pushing the boundaries of NLP.\n",
      "\n",
      "Therefore, while transformers have made significant contributions and are widely considered powerful, they represent one important piece of the deep learning landscape, and it would be misleading to claim any single algorithm as the \"most advanced\" in all aspects of deep learning. The field is rich with ongoing research and advancements, continuously pushing the boundaries of what is possible.\n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo',\n",
    "                 temperature=1)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history',\n",
    "                                  return_messages=True)\n",
    "\n",
    "prompt = ChatPromptTemplate(input_variables=['content'],\n",
    "                            messages=[SystemMessage(content='You are chatbot having a conversation with a human.'),\n",
    "                                      MessagesPlaceholder(variable_name='chat_history'), # where memory will be sorted\n",
    "                                      HumanMessagePromptTemplate.from_template('{content}')])\n",
    "\n",
    "chain = LLMChain(llm=llm,\n",
    "                 prompt=prompt,\n",
    "                 memory=memory,\n",
    "                 verbose=True)\n",
    "\n",
    "while True:\n",
    "    content = input('Your prompt: ')\n",
    "    if content in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "\n",
    "    response = chain.run({'content': content})\n",
    "    print(response)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving chat session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import FileChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are chatbot having a conversation with a human.\n",
      "Human: light speed in vacuum\n",
      "AI: The speed of light in a vacuum is approximately 299,792,458 meters per second (or about 186,282 miles per second). It's the ultimate speed limit in the universe, according to the theory of relativity.\n",
      "Human: what about the light speed in water?\n",
      "AI: The speed of light in a medium, such as water, is slower than its speed in a vacuum. In water, the speed of light is approximately 225,000 kilometers per second (or about 140,000 miles per second). The exact speed of light in any medium is determined by its refractive index, which is a measure of how much the medium slows down the speed of light compared to a vacuum.\n",
      "Human: How long dos it take to travel from Sun to Earth?\n",
      "AI: The distance between the Sun and the Earth, on average, is about 149.6 million kilometers (93 million miles). If we consider the speed of light in a vacuum, which is approximately 299,792 kilometers per second, we can calculate the time it takes for light to travel from the Sun to the Earth.\n",
      "\n",
      "Using the formula: time = distance / speed\n",
      "\n",
      "Time = 149.6 million kilometers / 299,792 kilometers per second\n",
      "Time ≈ 499.0 seconds\n",
      "\n",
      "Therefore, it takes around 499 seconds, or about 8 minutes and 19 seconds, for light to travel from the Sun to the Earth.\n",
      "Human: what did I ask you?\n",
      "AI: You asked how long it takes to travel from the Sun to the Earth.\n",
      "Human: Tell all questions I have asked.\n",
      "AI: Certainly! Here are the questions you have asked so far:\n",
      "\n",
      "1. \"Light speed in vacuum?\"\n",
      "2. \"What about the light speed in water?\"\n",
      "3. \"How long does it take to travel from the Sun to Earth?\"\n",
      "4. \"What did I ask you?\"\n",
      "\n",
      "Is there anything else you would like to know or discuss?\n",
      "Human: Explain how transformers in deep learning works\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Transformers are a type of deep learning model that has gained significant popularity in natural language processing (NLP) tasks. They were introduced in a 2017 paper titled \"Attention is All You Need\" by Vaswani et al.\n",
      "\n",
      "In traditional deep learning models, such as recurrent neural networks (RNNs), information flows sequentially from one step to the next. However, transformers take a different approach by utilizing a mechanism called self-attention.\n",
      "\n",
      "In transformers, the input is divided into sequences of fixed-length vectors called tokens. Each token is accompanied by an embedding, which captures its semantic meaning. The core idea behind transformers is the self-attention mechanism, which allows the model to weigh the importance of different tokens when processing the input sequence.\n",
      "\n",
      "The self-attention mechanism calculates attention weights for each token in relation to all other tokens in the input sequence. These weights reflect the relevance or importance of each token concerning the others. By using these attention weights, the model can focus more on tokens that are most informative for the current task.\n",
      "\n",
      "The transformer architecture consists of multiple layers of self-attention and feed-forward neural networks. In each layer, the self-attention mechanism captures the relationships between tokens, and the feed-forward networks process the attended representations to obtain more abstract features. These layers can be stacked to provide a deeper and more expressive model.\n",
      "\n",
      "During training, transformers employ a method called \"masked self-attention\" to prevent the model from attending to future tokens and maintain causality. This means the model can process each token without having access to subsequent tokens, making it suitable for tasks like machine translation where output is generated one token at a time.\n",
      "\n",
      "After training, transformers can be used for various NLP tasks such as machine translation, question answering, sentiment analysis, and more. The attention mechanism allows the model to capture long-range dependencies efficiently, making transformers capable of processing inputs of variable length and achieving state-of-the-art performance in many tasks.\n",
      "\n",
      "In summary, transformers in deep learning work by utilizing self-attention mechanisms to capture relationships between input tokens, allowing the model to weigh their importance and efficiently process long-range dependencies, making them powerful for various natural language processing tasks.\n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo',\n",
    "                 temperature=1)\n",
    "\n",
    "history = FileChatMessageHistory('chat_history.json')\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history',\n",
    "                                  chat_memory=history,\n",
    "                                  return_messages=True)\n",
    "\n",
    "prompt = ChatPromptTemplate(input_variables=['content'],\n",
    "                            messages=[SystemMessage(content='You are chatbot having a conversation with a human.'),\n",
    "                                      MessagesPlaceholder(variable_name='chat_history'), # where memory will be sorted\n",
    "                                      HumanMessagePromptTemplate.from_template('{content}')])\n",
    "\n",
    "chain = LLMChain(llm=llm,\n",
    "                 prompt=prompt,\n",
    "                 memory=memory,\n",
    "                 verbose=True)\n",
    "\n",
    "while True:\n",
    "    content = input('Your prompt: ')\n",
    "    if content in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "\n",
    "    response = chain.run({'content': content})\n",
    "    print(response)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_kernel",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
