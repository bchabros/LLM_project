{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63e90228-71a8-4072-aaba-eebeb636f873",
   "metadata": {},
   "source": [
    "# Splitting and Embedding Text Using LangChain\n",
    "https://python.langchain.com/docs/modules/data_connection/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "101758dc-9837-47b2-b5fa-3b79b2d2bffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20563b5-bc8f-42a3-8359-3904856f0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "008349f1-ac81-4f0c-8f3e-64ffb51eaf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('/Users/Chabi/Documents/LLM_project/LangChain_Pinecone_OpenAI/99-Data/churchill_speech.txt') as f:\n",
    "    churchil_speech = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f6d8b0-231c-4864-80ac-7cc7aeb6109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100,\n",
    "                                               chunk_overlap=20,\n",
    "                                               length_function=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e4646b-7c29-4859-815f-5ac50920946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.create_documents([churchil_speech])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c3c318-3c41-4b21-99a0-3f4412fde4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='From the moment that the French defenses at Sedan and on the Meuse were broken at the end of the'\n"
     ]
    }
   ],
   "source": [
    "print(chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d5d2d3-3bde-47a9-b8a5-c72efef74f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the moment that the French defenses at Sedan and on the Meuse were broken at the end of the\n"
     ]
    }
   ],
   "source": [
    "print(chunks[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58efc2a7-ac22-4fa9-93a9-7a60a59c779b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 300\n"
     ]
    }
   ],
   "source": [
    "print(f'Chunks: {len(chunks)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d7715b8-b19a-4a6d-a782-0eb2fb29696b",
   "metadata": {},
   "source": [
    "### Embedding Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4314f01d-f689-451d-bd33-e28146c641dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004 :.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8a49bad-4d6a-4e90-bbc2-87a58bf8833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 4820\n",
      "Embedding Cost in USD: 0.001928\n"
     ]
    }
   ],
   "source": [
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda03e50-a5b7-4768-9cba-27ee8ebb1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/langchain-pinecone/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602be542-a6c1-4f20-8be1-81b5449c6460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = embeddings.embed_query('abc')\n",
    "len(vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96ab86c3-412b-4aec-85f0-54faedc80cb9",
   "metadata": {},
   "source": [
    "# Inserting the Embedding into Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "468bc61a-021f-463c-80b9-3c6e3da71fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "\n",
    "pc = pinecone.Pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "785ca9db-1007-4980-8362-f53409abfac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...Done\n"
     ]
    }
   ],
   "source": [
    "# free version only one index \n",
    "# deleting all indexes\n",
    "indexes = pc.list_indexes().names()\n",
    "for i in indexes:\n",
    "    print('Deleting all indexes ...', end='')\n",
    "    pc.delete_index(i)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c37fbfad-2d60-4453-9aee-8d13a6c18ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index churchill-speech\n",
      "Index created! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# creating an index\n",
    "from pinecone import ServerlessSpec\n",
    "index_name = 'churchill-speech'\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f'Creating index {index_name}')\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    )\n",
    "    print('Index created! ðŸ˜Š')\n",
    "else:\n",
    "    print(f'Index {index_name} already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6361030c-2c59-42f8-814f-798d79222243",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Pinecone.from_documents(chunks,\n",
    "                                       embeddings,\n",
    "                                       index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3ed2a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 300}},\n",
       " 'total_vector_count': 300}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03d1b4e5-05d6-4c72-b8d2-24d4fb6eb601",
   "metadata": {},
   "source": [
    "# Ask Question (Similarity Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c8bffd6-3e96-4fc9-93b0-8739d9b28421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and'), Document(page_content='front, now on that, fighting'), Document(page_content='end, we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing'), Document(page_content='When we consider how much greater would be our advantage in defending the air above this Island')]\n"
     ]
    }
   ],
   "source": [
    "query = 'Where should we fight?'\n",
    "result = vector_store.similarity_search(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f44e5592-7c92-40d0-857a-9d2811a5e65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and\n",
      "--------------------------------------------------\n",
      "front, now on that, fighting\n",
      "--------------------------------------------------\n",
      "end, we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing\n",
      "--------------------------------------------------\n",
      "When we consider how much greater would be our advantage in defending the air above this Island\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    print(r.page_content)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21fa172b",
   "metadata": {},
   "source": [
    "## Answering in Natural Language using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90c92e28-0961-4b0c-9b56-8bd3cd96a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aa20a16-ffbf-4c14-89f6-bcb17d0f0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM with the specified model and temperature\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.2)\n",
    "\n",
    "# Use the provided vector store with similarity search and retrieve top 3 results\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "\n",
    "# Create a RetrievalQA chain using the defined LLM, chain type 'stuff', and retriever\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab788a9b-8904-4145-a9dd-b59bb1305d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Answer only from the provided input. Where should we fight?', 'result': 'We shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields.'}\n"
     ]
    }
   ],
   "source": [
    "query = 'Answer only from the provided input. Where should we fight?'\n",
    "answer = chain.invoke(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d2028e8-7304-4913-8f21-54f7086d4b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Who was the king of Belgium at that time?', 'result': 'The king of Belgium at that time was King Leopold.'}\n"
     ]
    }
   ],
   "source": [
    "query = 'Who was the king of Belgium at that time?'\n",
    "answer = chain.invoke(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9beb60c1-efdf-45b9-a97e-d30d6d3dbbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What about the French Armies??', 'result': 'The French Armies were involved in the fighting during the battle, and they were holding the territory that was being contested. Additionally, a French Army was created to advance across the Somme with great strength during the battle.'}\n"
     ]
    }
   ],
   "source": [
    "query = 'What about the French Armies??'\n",
    "answer = chain.invoke(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2807e6-f270-4ad4-88cb-6a3c88a92cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d2466-b66b-4782-b9bd-8258a7665e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-pinecone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
